{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import sys\n",
    "#set openAI api key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain.chains.chat_vector_db.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "import json\n",
    "from langchain.vectorstores import Pinecone\n",
    "# LLM wrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import OpenAI\n",
    "\n",
    "from langchain import SerpAPIWrapper, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "# Helper function for printing docs\n",
    "import textwrap\n",
    "\n",
    "def pretty_text(text):\n",
    "    wrapped_text = textwrap.wrap(text, width=100)\n",
    "    for line in wrapped_text:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x160a5f390>, search_type='similarity', search_kwargs={'k': 100, 'include_metadata': True})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_embedding_db(index_name):\n",
    "    from langchain.vectorstores import FAISS\n",
    "    # You may need to import the embeddings model depending on your application's structure\n",
    "    # from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.load_local(index_name, embeddings)\n",
    "    return db\n",
    "\n",
    "db = load_embedding_db(\"faiss_index_1000_200_1000papers\")\n",
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\"k\":100, \"include_metadata\": True})\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor, LLMChainFilter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\". \")\n",
    "# redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[relevant_filter]\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor, base_retriever=retriever)\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are Dr. Origins, a specialist in Galactic Astronomy. Your expertise lies in reading and critically interpreting astronomy papers to generate innovative, research-based ideas. \n",
    "Every idea should commence with \"I propose...\".\n",
    "\n",
    "Guidelines:\n",
    "1. Base your ideas on scientifically recognized theories and principles.\n",
    "2. Your ideas should be feasibly verifiable and provide avenues for further exploration or research in Galactic Astronomy.\n",
    "3. Abstain from making overly speculative claims or assertions that cannot be empirically tested.\n",
    "4. Always accurately reference established theories, observational data, or universally accepted astronomical concepts. Do not misrepresent or fabricate scientific references. If you are unsure about a reference, do not use it.\n",
    "5. Clearly distinguish your ideas from referenced material. Explain how the referenced research inspired your idea.\n",
    "6. Learn from feedback. Improve and adjust your proposal according to received input.\n",
    "7. Use less than 250 words.\n",
    "\n",
    "In response to a human query, generate an informed, precise, and critical response, ensuring your answer's clarity and originality. \n",
    "\n",
    "Context: {context}\n",
    "Human: {question}\n",
    "Dr. Origins: \"\"\"\n",
    " \n",
    "\n",
    "DRC_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "doc_template = \"\"\"--- document start ---\n",
    "citation: {citation}\n",
    "content:{page_content}\n",
    "--- document end ---\n",
    "\"\"\"\n",
    "\n",
    "ASTRO_DOC_PROMPT = PromptTemplate(\n",
    "    template=doc_template,\n",
    "    input_variables=[\"page_content\", \"citation\"],\n",
    ")\n",
    "\n",
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "\n",
    "model_name = \"gpt-4\"\n",
    "llm_qg = ChatOpenAI(temperature=0.2, model_name=model_name)\n",
    "\n",
    "\n",
    "TEMP = 0.7\n",
    "llm = ChatOpenAI(temperature=TEMP, model_name=model_name)\n",
    "\n",
    "question_generator = LLMChain(llm=llm_qg, prompt=CONDENSE_QUESTION_PROMPT) # this is the question generator, i probably need to change it to another model instance\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=DRC_PROMPT, document_prompt=ASTRO_DOC_PROMPT)\n",
    "\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, memory_key=\"chat_history\", return_messages=True, output_key=\"answer\")\n",
    "\n",
    "app_retriever = compression_retriever\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=app_retriever,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    max_tokens_limit=7000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8454d8a839514f605fce6bc8db8f217e in your message.).\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "The proposal to investigate the vertical distribution of stars in the\n",
    "Milky Way's disk using Gaia data and spectroscopic surveys has potential but needs to address some\n",
    "limitations and weaknesses. These include: 1. Providing a clear methodology for data integration,\n",
    "considering the complex and often incompatible selection functions of different surveys. 2. Defining\n",
    "the sample selection criteria to ensure the reliability of the results. 3. Addressing the\n",
    "uncertainties in determining individual stellar metal abundances and proper motions. 4.\n",
    "Disentangling the contributions of in-situ star formation and external accretion events, considering\n",
    "the complexity of the Galactic disk's structure and the interplay between internal and external\n",
    "processes. 5. Providing a detailed description of how the results will be compared with simulations\n",
    "for validating the findings and testing theories of Galactic disk formation.\",      \"question\": \"Can\n",
    "you revise the proposal to address these limitations and provide a more detailed methodology,\n",
    "including data integration, sample selection criteria, handling uncertainties, disentangling\n",
    "contributions of different processes, and comparing results with simulations to ensure the validity\n",
    "and reliability of the results?\n",
    "\"\"\"\n",
    "result = chain({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Number: 5699\n",
      "I propose an investigation of the vertical distribution of stars in the Milky Way's disk using Gaia\n",
      "data and spectroscopic surveys, such as APOGEE, GALAH, Gaia-ESO, and LAMOST, with an enhanced\n",
      "methodology to address limitations and improve the overall reliability of the results.  1. Data\n",
      "Integration: Combine the Gaia data, including parallaxes, proper motions, and radial velocities,\n",
      "with complementary information from spectroscopic surveys to create a comprehensive dataset for\n",
      "analysis. This will provide a more complete picture of the stellar distribution and kinematics in\n",
      "the Milky Way's disk (Gaia Collaboration et al. 2018; Yang et al. 2020).  2. Sample Selection\n",
      "Criteria: Establish stringent sample selection criteria to minimize biases and ensure a\n",
      "representative sample of stars across different regions of the Milky Way's disk (Boubert et al.\n",
      "2020). This may include selecting stars based on their spectral type, distance from the Sun,\n",
      "apparent magnitude, and spatial distribution.  3. Handling Uncertainties: Account for uncertainties\n",
      "in measurements such as parallax, proper motion, and radial velocity by adopting robust statistical\n",
      "techniques, like the Poisson likelihood function (Everall et al. 2021c). This will help in\n",
      "accurately estimating the vertical distribution parameters and their uncertainties.  4.\n",
      "Disentangling Contributions of Different Processes: Develop a comprehensive dynamical model that\n",
      "takes into account various processes that may affect the vertical distribution of stars, such as\n",
      "radial migration, heating, and interactions with dark matter (Rix and Bovy 2013). This will help in\n",
      "isolating the effects of each process and better understanding their relative contributions to the\n",
      "observed distribution.  5. Comparing Results with Simulations: Validate the developed model by\n",
      "comparing the inferred parameters with those obtained from N-body simulations and mock Gaia samples\n",
      "(Rix and Bovy 2013; Everall et al. 2021c). This will ensure the reliability and robustness of the\n",
      "results and help identify any potential biases or systematics in the analysis.  By incorporating\n",
      "these aspects into the methodology, the investigation of the vertical distribution of stars in the\n",
      "Milky Way's disk using Gaia data and spectroscopic surveys will yield more accurate, reliable, and\n",
      "robust results, providing valuable insights into the structure, formation, and evolution of our\n",
      "Galaxy.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def process_and_save_result(result, name):\n",
    "    # Generate a random number\n",
    "    random_number = random.randint(1000, 10000)\n",
    "    print(f\"Random Number: {random_number}\")\n",
    "\n",
    "    # Save result and answer\n",
    "    answer = result[\"answer\"]\n",
    "    pretty_text(answer)\n",
    "    with open(f'answer_{name}.json', 'w') as fp:\n",
    "        json.dump(answer, fp)\n",
    "\n",
    "    # Extract and print the metadata\n",
    "    metadata = []\n",
    "    for item in result[\"source_documents\"]:\n",
    "        metadata.append(item.metadata)\n",
    "        #print(item.metadata)\n",
    "        \n",
    "    # Save the metadata\n",
    "    with open(f'metadata_{name}.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "# Now you can simply call this function with your result and a name\n",
    "process_and_save_result(result, '1000_07_e4_a2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df):\n",
    "    #subselect only Content, Citation and meta_key\n",
    "    #make df['ArxivID'] into string\n",
    "    df['ArxivID'] = df['ArxivID'].astype(str)   \n",
    "    df = df[['Content', 'citation', 'meta_key']]\n",
    "    return df\n",
    "\n",
    "def prepare_and_load_df(df_path):\n",
    "    df = pd.read_csv(df_path)\n",
    "    df['ArxivID'] = df['ArxivID'].astype(str)   \n",
    "    df = df[['Content', 'citation', 'meta_key']]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = prepare_and_load_df('papers/df_arxiv_100_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result_for_temperature(temp, query):\n",
    "    model_name = \"gpt-4\"\n",
    "\n",
    "    # Adjust temperature of models\n",
    "    llm_qg = ChatOpenAI(temperature=0.3)\n",
    "    llm = ChatOpenAI(temperature=temp, model_name=model_name)\n",
    "\n",
    "    question_generator = LLMChain(llm=llm_qg, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "    doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=DRC_PROMPT, document_prompt=ASTRO_DOC_PROMPT)\n",
    "\n",
    "    memory = ConversationSummaryBufferMemory(llm=llm, memory_key=\"chat_history\", return_messages=True, output_key=\"answer\")\n",
    "    app_retriever = compression_retriever\n",
    "\n",
    "    chain = ConversationalRetrievalChain(\n",
    "        retriever=app_retriever,\n",
    "        question_generator=question_generator,\n",
    "        combine_docs_chain=doc_chain,\n",
    "        memory=memory,\n",
    "        return_source_documents=True,\n",
    "        max_tokens_limit=7500,\n",
    "    )\n",
    "\n",
    "    # Query\n",
    "    result = chain({\"question\": query})\n",
    "\n",
    "    # Get meta_keys\n",
    "    meta_keys = [item.metadata['meta_key'] for item in result['source_documents']]\n",
    "\n",
    "    # Create a dictionary with the data\n",
    "    data = {'temp': temp, \n",
    "            'history': len(result['chat_history'])//2-1, \n",
    "            'question': result['question'], \n",
    "            'result': result['answer'], \n",
    "            'meta_key': meta_keys}\n",
    "\n",
    "    return data\n",
    "\n",
    "# List of desired temperatures\n",
    "temperatures = [0.1, 0.3, 0.5, 0.7, 0.9]  \n",
    "\n",
    "query = \"\"\"Drawing from the literature you have access to, propose a novel idea in Galactic Astronomy that can be tested with current or future observations.\"\"\"\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate through the temperatures and append results to the list\n",
    "for temp in temperatures:\n",
    "    result = generate_result_for_temperature(temp, query)\n",
    "    results.append(result)\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df = pd.DataFrame(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrofm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
